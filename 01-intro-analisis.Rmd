# Introducción al análisis de datos

> "Exploratory data analysis can never be the whole story, but nothing
else can serve as the foundation stone --as the first step." --- John Tukey

```{r, message = FALSE, echo = FALSE, include = FALSE}
knitr::opts_chunk$set(
    comment = "#>",
    collapse = TRUE,
    fig.align = "center", 
    error = FALSE,
    cache = FALSE
)
ggplot2::theme_set(ggplot2::theme_light())
```

Muchas veces se le llama **análisis descriptivo** a una combinación
de resúmenes, gráficas y tablas cuyos propósitos pueden englobarse en:

1. **Validación de datos**. Verificar si en los datos 
existen o no valores imposibles o "extraños" para algunas mediciones,
si existen datos faltantes, o existen otros posibles "defectos", 
irregularidades o sorpresas.

2. Mostrar **estadísticas descriptivas**. Mostrar algunas resúmenes útiles y compactos 
de las mediciones
que tenemos: cuáles son valores o rangos de valores comunes, cuáles son los
extremos de las mediciones y así sucesivamente.

3. **Análisis exploratorio**. Buscamos gráficas o resúmenes que muestren patrones 
más importantes y sustanciales,
como la relación entre distintas partes de los datos, qué partes de la variación en 
los datos son tentativamente atribuibles a ciertos factores y qué partes no.

De forma más moderna, estos tres puntos pueden englobarse dentro del último término:
*análisis exploratorio*. Esta fase del análisis de datos es fundamental, como
la cita de Tukey explica arriba, y se caracteríza por un *enfoque de detective*:
quizá tenemos algunas preguntas, algunas sospechas, y en esta fase acumulamos indicios
que nos indiquen caminos prometedores de investigación.

En contraste, tenemos el *análisis confirmatorio*, que busca validar hipótesis
o dar respuestas correctamente cuantificadas en cuanto a su incertidumbre o grado 
de error. En esta parte somos más *jueces* que detectives, y utilizamos más maquinaria
matemática (teoría de probabilidad) para especificar con claridad nuestros supuestos y 
poder hacer cálculos cuidadosos, generalmente basados en algún tipo de aleatorización.

Ninguno de los dos tipos de análisis funciona muy bien sin el otro, 
(@tukeyexpconf) y explicaremos
por qué un poco más adelante. Por el momento, para ilustrar el enfoque exploratorio, 
comenzaremos con datos que podemos
describir de manera completa y efectiva sin necesidad de hacer resúmenes o aplicar
técnicas avanzadas.


## Ejemplo: nacimientos

Consideremos una parte de los datos de nacimientos por día del INEGI de 1999 a 2016.
Consideraremos sólo tres meses: enero a marzo de 2016. Estos datos, por su tamaño,
pueden representarse de manera razonablemente efectiva en una visualización de serie 
de tiempo

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(lubridate)
library(kableExtra)

nacimientos <- read_rds("datos/nacimientos/natalidad.rds") |>
  ungroup() |> 
  filter(year(fecha) == 2016, month(fecha) <= 3)
```

Examinamos partes del contenido de la tabla:

```{r}
tab_1 <- nacimientos |> 
  select(fecha, n) |> 
  slice_head(n = 5)
tab_2 <- nacimientos |> 
  select(fecha, n) |> 
  slice_tail(n = 5)
kable(list(tab_1, tab_2)) |> kable_styling()
```

En un examen rápido de estos números no vemos nada fuera de orden. Los datos
tienen forma de serie de tiempo regularmente espaciada (un dato para cada día). Podemos graficar
de manera simple como sigue:

```{r, fig.width=9, fig.height = 2.5}
ggplot(nacimientos, aes(x = fecha, y = n)) +
   geom_point() +
   geom_line() + 
   scale_x_date(breaks = "1 week", date_labels = "%d-%b") 
```

Esta es una descripción de los datos, que quizá no es muy compacta pero
muestra varios aspectos importantes. En este caso notamos
algunos patrones que saltan a la vista. Podemos marcar los domingos de cada semana:

```{r, fig.width=9, fig.height = 2.5}
domingos_tbl <- nacimientos |> 
   filter(weekdays(fecha) == "domingo")
ggplot(nacimientos, aes(x = fecha, y = n)) +
   geom_vline(aes(xintercept = fecha), domingos_tbl, colour = "salmon") +
   geom_point() +
   geom_line() + 
   scale_x_date(breaks = "1 week", date_labels = "%d-%b") 
```

Observamos que los domingos ocurren menos nacimientos y los sábados también ocurren relativamente
menos nacimentos. ¿Por qué crees que sea esto?

Adicionalmente a estos patrones observamos otros aspectos interesantes:

- El primero de enero hay considerablemente menos nacimientos de los que esperaríamos
para un viernes. ¿Por qué?
- El primero de marzo hay un exceso de nacimientos considerable. ¿Qué tiene de especial
este primero de marzo?
- ¿Cómo describirías lo que sucede en la semana que comienza el 21 de marzo? ¿Por qué crees que pase eso?
- ¿Cuáles son los domingos con más nacimientos? ¿Qué tienen de especial y qué explicación puede tener?

La confirmación de estas hipótesis, dependiendo de su forma, 
puede ser relativamente simple (por ejemplo ver una serie más larga de domingos comparados
con otros días de la semana) hasta muy 
compleja (investigar preferencias de madres, de doctores o de hospitales, costumbres y actitudes,
procesos en el registro civil, etc.)

## Procesos generadores de datos {-}

De este primer ejemplo donde usamos una gráfica simple:

```{block2, type="comentario"}
Nótese que en todas estas preguntas hemos tenido que recurrir a conocimientos generales
y de dominio para interpretar y hacer hipótesis acerca de
lo que vemos en la gráfica. Una visión descontextualizada no tiene mucha utilidad.
Las explicaciones son típicamente complejas e intervienen distintos
aspectos del comportamiento de actores, sistemas, y métodos de recolección de datos involucrados.

Al conjunto de esos aspectos que determinan los datos que finalmente observamos le
llamamos el **proceso generador de datos**.
```

El análisis de datos en general busca entender las partes importantes del proceso
que los generó. En el análisis descriptivo y exploratorio buscamos iluminar ese proceso,
proponer hipótesis y buscar caminos interesantes para investigar, ya sea con
técnicas cuantitativas o con trabajo de campo (como sugiere el título de artículo
de David A. Friedman: [Statistical Models and Shoe Leather](https://psychology.okstate.edu/faculty/jgrice/psyc5314/Freedman_1991A.pdf)).

Con la teoría de probabilidades podemos modelar más explícitamente partes de
estos procesos generadores de datos, especialmente cuando controlamos parte
de ese proceso generador mediante técnicas estadísticas de diseño, por ejemplo,
usando aleatorización.


## Ejemplo (cálculos renales) {-}

En este ejemplo también intentaremos mostrar los datos completos sin intentar
resumir.

Este es un estudio real acerca de tratamientos para cálculos renales 
(@kidney94). Pacientes se asignaron de una forma no controlada
a dos tipos de tratamientos para reducir
cálculos renales. Para cada paciente, conocemos el el tipo de ćalculos que tenía
(grandes o chicos) y si el tratamiento tuvo éxito o no.

La tabla original se ve como sigue (muestreamos algunos renglones):

```{r, message = FALSE}
calculos <- read_csv("./datos/kidney_stone_data.csv")
names(calculos) <- c("tratamiento", "tamaño", "éxito")
calculos <- calculos |> 
   mutate(tamaño = ifelse(tamaño == "large", "grandes", "chicos")) |> 
   mutate(resultado = ifelse(éxito == 1, "mejora", "sin_mejora")) |> 
   select(tratamiento, tamaño, resultado)
nrow(calculos)
calculos |> 
   sample_n(20) |> 
   kable()
```

Aunque estos datos contienen información de 700 pacientes (cada renglón es un paciente),
los datos pueden resumirse sin pérdida de información contando como sigue:

```{r}
calculos_agregada <- calculos |> 
   group_by(tratamiento, tamaño, resultado) |> 
   count()
calculos_agregada |> kable()
```
Este resumen no es muy informativo, pero al menos vemos qué valores aparecen en 
cada columna de la tabla. Como en este caso nos interesa principalmente la tasa de éxito
de cada tratamiento, podemos mejorar mostrando como sigue:

```{r}
calculos_agregada |> pivot_wider(names_from = resultado, values_from = n) |> 
   mutate(total = mejora + sin_mejora) |> 
   mutate(prop_mejora = round(mejora / total, 2)) |> 
   select(tratamiento, tamaño, total, prop_mejora) |> 
   arrange(tamaño) |> 
   kable()
```

Esta tabla descriptiva es una reescritura de los datos, y no hemos resumido nada todavía.
Sin embargo, esta tabla es apropiada para empezar a contestar la pregunta:

- ¿Qué indican estos datos acerca de qué tratamiento es mejor? ¿Acerca del tamaño
de cálculos grandes o chicos?

Supongamos que otro analista decide comparar los pacientes que recibieron cada
tratamiento, ignorando la variable de tamaño:


```{r}
calculos |> group_by(tratamiento) |> 
   summarise(prop_mejora = mean(resultado == "mejora") |> round(2)) |> 
   kable()
```

y parece ser que el tratamiento $B$ es mejor que el $A$. Esta es una 
paradoja (un ejemplo de la [paradoja de Simpson](https://es.wikipedia.org/wiki/Paradoja_de_Simpson)) . Si un médico no sabe 
que tipo de cálculos tiene el paciente,
¿entonces debería recetar $B$? ¿Si sabe debería recetar $A$? Esta discusión parece
no tener mucho sentido.

Podemos investigar por qué está pasando esto considerando la siguiente tabla, que
solo examina cómo se asignó el tratamiento dependiendo del tipo de cálculos de cada paciente:

```{r}
calculos |> group_by(tratamiento, tamaño) |> count() |> 
   kable()
```

Nuestra hipótesis aquí es que  la decisión de qué tratamiento usar depende del tamaño
de los cálculos. En este caso, por alguna razón se prefiere utilizar el tratamiento $A$ para
cálculos grandes, y $B$ para cálculos chicos. Esto quiere decir que
en la tabla total *el tratamiento $A$ está en desventaja porque se usa en
casos más difíciles*, pero el tratamiento $A$ parece ser en general mejor.

Igual que en el ejemplo anterior, los resúmenes descriptivos están acompañados
de hipótesis acerca del *proceso generador de datos*, y esto ilumina lo que estamos
observando y nos guía hacia descripciones provechosas de los datos. Las explicaciones
no son tan simples y, otra vez, interviene el comportamiento de doctores, 
tratamientos, y distintos tipos de padecimientos. 


## Inferencia y predicción

En los ejemplos anteriores, sólo vimos muestras de datos (algunos pacientes, algunas fechas).
Nuestras descripciones son, estrictamente hablando, válidas para esa muestra de los datos.

Si quisiéramos generalizar a la población de pacientes con cálculos (quizá en
nuestra muestra el tratamiento A parece mejor, pero 
¿qué podemos decir para la población de pacientes), o quisiéramos predecir
cómo van a ser los nacimientos en 2021,  requerimos otro tipo de análisis: 
**inferencial y predictivo**. Estos dos tipos de análisis, centrales en la estadística, buscan
establecer condiciones para poder generalizar de nuestra muestra a datos no observados (otros
pacientes, nacimientos en el futuro), y cuantificar qué tan bien o mal podemos hacerlo.

Para llegar a este tipo de análisis, generalmente tenemos que comenzar con el análisis
exploratorio, y con la comprensión de los fundamentos del proceso generador asociado
a nuestros datos. En algunos casos, veremos que es posible usar herramientas matemáticas
para modelar aspectos de nuestro proceso generador de datos, que cuando válidas, nos permiten
generalizar y ampliar apropiadamente el rango de nuestras conclusiones.

La herramienta básica para construir, entender y operar con estos modelos es la **teoría
de probabilidad**, que veremos más adelante.


## Ejercicio: admisiones de Berkeley {-}

Consideramos ahora los siguientes datos de admisión a distintos departamentos
de Berkeley en 1975:

```{r}
data("UCBAdmissions")
adm_original <- UCBAdmissions |> as_tibble() |> 
   pivot_wider(names_from = Admit, values_from = n) 
adm_original |> knitr::kable()
```

Con algo de manipulación podemos ver tasas de admisión para *Male* y *Female*, y
los totales de cada grupo que solicitaron en cada Departamento.

```{r}
adm_tbl <- adm_original |> 
   mutate(prop_adm = round(Admitted / (Admitted + Rejected), 2), total = Admitted + Rejected) |> 
   select(Gender, Dept, prop_adm, total) |> 
   pivot_wider(names_from = Gender, values_from = prop_adm:total)
adm_tbl |> knitr::kable()
```

Y complementamos con las tasas de aceptación a total por género, y tasas de aceptación
por departamento:

```{r}
adm_original |> group_by(Gender) |> 
   summarise(Admitted = sum(Admitted), Rejected = sum(Rejected)) |> 
   mutate(prop_adm = round(Admitted / (Admitted + Rejected),2)) |> 
   kable()
```
```{r}
adm_original |> group_by(Dept) |> 
   summarise(Admitted = sum(Admitted), Rejected = sum(Rejected)) |> 
   mutate(prop_adm = round(Admitted / (Admitted + Rejected),2)) |> 
   kable()
```

- ¿Qué observas acerca de las tasas de admisión en cada departamento, diferenciadas por
género? ¿Qué tiene qué ver con el número de personas que solicitan en cada departamento?
- Esta es una tabla *descriptiva*. Sin embargo, tiene que ser entendida en el contexto de 
los datos y su generación. ¿Qué hipótesis importantes sugieren estos datos?
¿Por qué hay tanta diferencia de género de solicitudes en algunos departamentos?
¿Por qué es sorprendente o no las variaciones en tasas de aceptación de estudiantes de
cada género?

## Descripciones simples y problemas complejas (lectura)

Muchas veces se descarta al análisis descriptivo o exploratorio (al menos
en sus formas más crudas) como algo que no requiere
mucha habilidad técnica o conocimiento de dominio, o cuando se quiere evitar plantear 
hipótesis claras acerca de los datos que ayuden en su entendimiento.

En realidad el análisis descriptivo y exploratorio es crucial en el análisis
de datos en general, y tiene siempre
que venir acompañado de conocimiento de dominio, habilidad técnica, una mente crítica y
muchas veces ingenio y creatividad.

## ¿Qué preguntas formular? (lectura)

Existen algunas prácticas generales que utilizamos para hacer validaciones y
resúmenes simples de los datos que discutiremos más adelante. Por el momento, discutimos
las razones por las que estamos haciendo ese análisis en un principio.

En general, comenzamos con algunas preguntas básicas que quisiéramos contestar con los datos.
El análisis exploratorio juega un papel central para comenzar a responder:

- ¿Es razonable la pregunta que queremos contestar?
- ¿Podemos contestar la pregunta con los datos que tenemos?

Aunque estos dos incisos a veces parecen transparentes y simples de contestar,
generalmente no lo son: las preguntas que queremos contestar y los problemas
que queremos resolver usualmente son no triviales.

## Formulación de preguntas y respuestas (lectura)

El proceso de la ciencia de datos no va desde las preguntas
hasta las respuestas en un camino lineal.

En esta gráfica [Roger Peng](https://simplystatistics.org/2019/04/17/tukey-design-thinking-and-better-questions) hay tres caminos: uno es uno ideal que pocas veces sucede,
otro produce respuestas poco útiles pero es fácil, y otro es tortuoso pero que 
caracteriza el mejor trabajo de análisis de datos:


```{r, echo = FALSE, message = FALSE, fig.cap = "Adaptado de R. Peng: Tukey, design thinking and better questions"}
library(tidyverse)
puntos <- tibble(x = c(0.5, 1.2, 4, 4), y = c(0.5, 4, 0.5, 5),
                 etiqueta = c("Dónde\ncomenzamos\nrealmente", "Análisis de datos \n poco útil, de bajo impacto",  "Dónde creeemos \nque comenzamos", "Nuestra\nmeta "))

set.seed(211)

browniano <- tibble(x = 0.5 +  cumsum(c(0,rnorm(50, 0.03, 0.1))) ,
                    y = 0.5 +  cumsum(c(0, rnorm(50, 0.02, 0.2))))
puntos <- 
  bind_rows(puntos, tail(browniano, 1) |> 
              mutate(etiqueta = "¡¿terminamos!?"))

flechas <- 
  tibble(x = c(0.5, 4), y = c(0.5, 0.5), xend = c(1.2, 4), yend = c(4, 5))

ggplot(puntos, aes(x = x, y = y)) + 
    xlab("Calidad de la pregunta") +
    ylab("Peso de la evidencia") +
    theme(axis.text.x=element_blank(),
        axis.text.y=element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()) +
    geom_segment(data = flechas, aes(xend=xend, yend=yend),
                 arrow = arrow(length = unit(0.3, "inches"))) +
    geom_path(data = browniano) +
    geom_point(data = browniano) +
    geom_point(colour="red", size = 5) +
    geom_text(aes(label = etiqueta), vjust = -0.5, hjust = 1.1, size = 4.2) +
    #labs(caption = "Adaptado de R. Peng: Tukey, design thinking and better questions.") +
    xlim(c(-0.1 , 4)) + ylim(c(0,6))
    
```

El proceso típico involucra pasos como los siguientes, y es un proceso **no lineal**:

* Hacer preguntas de la materia que nos interesa
* Recolectar, consumir y procesar los datos para abordarla
* Explorar estos datos y evaluar su calidad
* Hacer análisis o modelos
* Reportar los resultados de forma adecuada y con esto resolver y replantear las preguntas importantes. 

Por ejemplo, evaluar la calidad de los datos puede llevar a replantear la necesidad de obtener más información o de hacer estudios específicos. Así también, los modelos pueden dar luz sobre las preguntas que los originan.




## ¿Cómo saber que vamos en el camino correcto? 
 
```{block2, type="comentario"}
¿Por dónde empezar el análisis descriptivo y exploratorio? ¿Cómo sabemos que
vamos por buen camino y qué hacer cuando sentimos que nos estancamos?
``` 
 
 
Comenzamos por discribir cuáles son los signos de calidad del análisis 
que piensa usarse como insumo para una decisión. 
Los principios del diseño analítico de Edward Tufte (@tufte06) son:

Los análisis exitosos:

1. Muestran y explotan **comparaciones**, diferencias y variación.
2. Tienden a ser **multivariados**: estudian conjuntamente más de 1 o 2 variables.
3. Muestran y explotan **estructura sistemática**, sugieren explicaciones. Cuando es posible,
aportan evidencia de causalidad.

También muy importantes pero en los que pondremos menos énfasis:

4. Datos y procesos están bien **documentados**. El análisis es reproducible y transparente.
5. Intentan **integrar** la evidencia completa: texto, explicaciones, tablas y
gráficas.

Y finalmente, el principio general:

6. La calidad, relevancia, e integridad del contenido y los datos son los que
al final sostienen al análisis - por sí mismos, **el uso de técnicas sofisticadas, algoritmos novedosos, uso o no de grandes datos, estilo de visualizaciones o presentaciones no son marcas o sellos de un análisis de datos exitoso**.

```{block2, type="comentario"}
Evaluar un análisis o resultado en estos seis puntos generalmente ayuda en el 
proceso de refinamiento de preguntas y respuestas.
```



## Gráfica de Minard

La ilustración que Tufte usa para mostrar excelencia en diseño analítico es
una [gráfica de Minard](https://en.wikipedia.org/wiki/Charles_Joseph_Minard) que sirve para entender la campaña de Napoleón (1812) 
en Rusia. Es un ejemplo atípico, pero representa bien los principios y también muestra 
la importancia del ingenio en la construcción de un anállsis:


```{r, echo = FALSE, fig.cap = "Marcha de Napoleón de Charles Minard. Tomado de Wikipedia"}
knitr::include_graphics("figuras/Minard.png")
```

```{block2, type="pregunta"}
¿Cómo satisface los principios del diseño analítico este gráfico?
```



## Ejemplo: Estados y calificaciones en SAT

Una pregunta típica y discutida es: 

```{block2, type="pregunta"}
¿Es cierto que los estados que gastan 
más en educación obtienen los mejores resultados en el SAT?
```

Aunque hay trabajo
considerable en definir estos términos, supongamos que tenemos el
[siguiente conjunto de datos](http://jse.amstat.org/datasets/sat.txt), que son
datos oficiales agregados por estado de Estados Unidos. Tenemos las variables
*sat*, por ejemplo, que es la calificación promedio de los alumnos en cada estado
(para 1997), y la variable *expend*, que es el gasto en miles de dólares
por estudiante en (1994 - 1995), además de algunas otras variables. 


```{r, message = FALSE}
library(skimr)
sat_tbl <- read_csv("datos/sat.csv")
skim(sat_tbl)
```

* ¿Cómo se ven las
calificaciones para estados por nivel de gasto?

Podemos
usar una gráfica de dispersión (más adelante veremos el suavizador que
se utiliza como línea de referencia):


```{r}
library(ggrepel)
ggplot(sat_tbl, aes(x = expend, y = sat, label = state)) + 
  geom_smooth(span = 1, method = "loess", se = FALSE) +
  geom_point(colour = "red", size = 2) + 
  geom_text_repel(colour = "gray50", size = 3) +
  xlab("Gasto por alumno (miles de dólares)") +
  ylab("Calificación promedio en SAT") 
```

Esta gráfica parece sugerir que los estados en donde el gasto es mayor, 
los resultados promedio del SAT no son tan buenos. Con esto, hemos
contestado la pregunta inicial. Este, sin embargo, es un análisis deficiente. Aunque la
técnica y la respuesta que arroja no son malas,
**la pregunta en su forma actual no es una pregunta muy bien planteada, interesante o útil**.


Después de examinar los datos, descubrimos una variable que puede ser importante: el porcentaje
de alumnos que tomó el SAT en el año dado (*frac*). Podemos agregar al análisis como sigue (el color más claro representa un mayor porcentaje):

```{r}
ggplot(sat_tbl, aes(x = expend, y = sat, label=state, colour = frac,
                 group = frac)) + 
  geom_point() + geom_text_repel(size = 3) +
  xlab("Gasto por alumno (miles de dólares)") +
  ylab("Calificación promedio en SAT") +
  scale_color_gradient(low = "orange", high = "black")
```


Vemos entonces por qué nuestra comparación inicial es relativamente pobre:
los estados con mejores resultados promedio en el SAT son aquellos donde una
fracción baja de los estudiantes toma el examen. La diferencia
es considerable (de menos de 20% de los alumnos hasta 80%), y la relación
es fuerte:

```{r, message = FALSE}
ggplot(sat_tbl, aes(x = frac, y = sat, label = state, colour = frac)) + 
  geom_point() +
    geom_smooth(method = "loess", span = 0.5, se = FALSE, 
                method.args = list(degree = 1), col = "blue") +
    annotate("text", x = 40, y = 1050, label = "Desempeño por \n encima del esperado", 
             colour = "gray30") +
    annotate("text", x = 15, y = 900, label = "Desempeño por \n abajo del esperado", 
             colour  = "gray30")  +
  theme(legend.position = "none") +
  scale_color_gradient(low = "coral1", high = "firebrick4")
```

Este patrón se observaría cuando los mejores alumnos en cada
estado son los que toman el SAT, aún cuando los estados fueran muy similares
y el gasto no tuviera efecto sobre el desempeño de los alumnos.

Tenemos entonces que replantear nuestra pregunta inicial.

- Pregunta original: ¿será cierto que los estados que gastan 
más en educación son los que obtienen los mejores resultados en el SAT?

- Nueva pregunta: Para estados donde un porcentaje similar de alumnos toma el SAT,
¿mayor gasto en educación está relacionado con mejores resultados en el SAT?

En este punto podemos hacer varias cosas. Una primera idea es intentar comparar
estados más similares en cuanto a la población de alumnos que asiste. Para esto, hacemos conglomerados con respecto a la fracción de alumnos que presentan el SAT. 

```{r}
library(broom)
set.seed(991)
sat_tbl$grupo <- 
  kmeans(sat_tbl |> select(frac), centers = 4,  nstart = 100, iter.max = 100) |>
  augment(sat_tbl) |> 
  pull(.cluster)
sat_tbl <- 
  sat_tbl |> group_by(grupo) |> 
  mutate(grupo_mediana_frac = round(median(frac))) |> 
  ungroup() |> 
  mutate(grupo_mediana_frac = factor(grupo_mediana_frac),
         grupo = as.numeric(grupo_mediana_frac))
sat_tbl |> 
   group_by(grupo) |>
   summarise(median_frac = median(frac)) |>
   arrange(median_frac) |>
   knitr::kable(format  =  "html",
               digits  =  2,
               format.args  =  list(decimal.mark  =  '.', big.mark  =  ",")) 
```

Incluyendo en nuestro análisis el porcentaje de participación, empezamos a ver otra historia. Ajustamos rectas de mínimos cuadrados en cada grupo como referencia:

```{r, message=FALSE}
ggplot(sat_tbl, aes(x = expend, y = sat, label=state, 
                colour = grupo_mediana_frac)) + 
  geom_point(size = 2) + 
  geom_smooth(method = "lm", se = F) +
  xlab("Gasto por alumno (miles)") +
  ylab("Calificación promedio en SAT") + 
  geom_text_repel(colour = "gray70", size = 2) 
```

Este análisis también nos sugiere que la pregunta requiere afinación adicional. 
Podríamos por ejemplo pensar:


- ¿En qué gastan las escuelas ese dinero? Quizá podemos considerar esos programas
particulares para dar una mejor respuesta.
- ¿Cómo serían los resultados individuales si se redujera el gasto del estado? ¿De qué cosas serían privadas en su escuela? 

Son preguntas más difíciles, pero vale más la pena hacer un esfuerzo con contestar
estas preguntas aunque sea aproximadamente, aún cuando no podamos dar una respuesta
precisa y/o simple.





